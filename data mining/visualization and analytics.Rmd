---
title: "BA_FINAL"
author: "phani varshitha"
date: "2023-12-19"
output:
  word_document: default
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Loading the required packages
```{r}
library("ISLR")
library("caret")
library("class")
library("e1071")
library("dplyr")
library("tidyverse")
library("ggplot2")
library("gmodels")
library("MASS")
library("broom")
library("modelr")
library("Hmisc")
library("missForest")
library("rpart")
library("rattle")
library("pROC")
library("ROCR")
library("cutpointr")
library("ROSE")
```

#Loading the data sets


#Loading the train dataset

```{r}
raw_data <- read.csv("C:/Users/varshitha/Downloads/Churn_Train.csv")
```

# Loading the test dataset
```{r}
load("C:/Users/varshitha/Downloads/Customers_To_Predict.RData")
```

#Cleaning and transforming the data

```{r}
#Removing Unnecessary Columns
Train_churn <- raw_data[,-c(1:3)]

#Re-coding few variables
Train_churn $churn <- ifelse(Train_churn $churn =="yes",1,0)
Train_churn $international_plan <- ifelse(Train_churn$international_plan =="yes",1,0)
Train_churn$voice_mail_plan <- ifelse(Train_churn$voice_mail_plan =="yes",1,0)

#Inputing NA Values
all_column_median <- apply(Train_churn,2,median, na.rm=T)
  
for(i in colnames(Train_churn))
Train_churn[,i][is.na(Train_churn[,i])] <- all_column_median[i]

#Converting integer to factor
Train_churn$churn <- as.factor(Train_churn$churn)

#Changing the order of the factor levels
Train_churn$churn <-  factor(Train_churn$churn,levels(Train_churn$churn)[c(2,1)])
```

#Partitioning the data 

```{r}
partition_data <- createDataPartition(Train_churn$churn,p=.75,list=F)

Train_Data <- Train_churn[partition_data,]
Validation_Data <- Train_churn[-partition_data,]
```

#Logistic Regression Model

```{r}
set.seed(125)
train_control <- trainControl(method = "repeatedcv",number=10,repeats = 3,savePredictions = 'final',classProbs = F)
 
lr.model <- train(churn~., data = Train_Data, method = "glm", family="binomial", metric="Accuracy", trControl = train_control)
```

#Decision Tree Model

```{r}
set.seed(765)
Dec_Tree.model <- rpart(churn~.,data=Train_Data,method="class")
```

#Testing the models over validation set

```{r}
#Predicting the logistic regression model built over the validation data to check the accuracy
lr_validate <- predict(lr.model,Validation_Data,type ="prob")
churn.lr.validate <- cbind(Validation_Data,lr_validate)

#Predicting the decision tree model built over the validation data to check the accuracy
dec_validate <- predict(Dec_Tree.model,Validation_Data,type ="prob")
churn.dec.validate <- cbind(Validation_Data,dec_validate)
```

#Optimal Threshold - Cut Off Point for Logistic Regression

```{r}
#Logistic Regression
ROC_pred_lr_test <- prediction(lr_validate[,1],churn.lr.validate$churn)

ROCR_perf_lr_test <- performance(ROC_pred_lr_test,'tpr','fpr')

acc_lr_perf <- performance(ROC_pred_lr_test,"acc")

optimal_cutoff_lr <-ROC_pred_lr_test@cutoffs[[1]][which.max(acc_lr_perf@y.values[[1]])]

#AUC Value
roc.curve(churn.lr.validate$churn, lr_validate[,1], plotit = F)
```

```{r}
#Decision Tree
ROC_pred_dec_test <- prediction(dec_validate[,1],churn.dec.validate$churn)

ROCR_perf_dec_test <- performance(ROC_pred_dec_test,'tpr','fpr')

acc_dec_perf <- performance(ROC_pred_dec_test,"acc")

ROC_pred_dec_test@cutoffs[[1]][which.max(acc_dec_perf@y.values[[1]])]

#AUC Value
roc.curve(churn.dec.validate$churn,dec_validate[,1], plotit = F)
```
#Re-Coding Variables - To run the CrossTable()

```{r}
#Setting the optimal cutoffs for all the models
#Logistic Regression Model
churn.lr.validate$prob <- as.factor(ifelse(churn.lr.validate$`1`>0.6705911,"yes","no"))
#Decision Tree Model
churn.dec.validate$prob <- as.factor(ifelse(churn.dec.validate$`1`>0.3076923,"yes","no"))

#Converting the churn column back to yes and no
churn.lr.validate$churn <- as.factor(ifelse(churn.lr.validate$churn==1,"yes","no"))
churn.dec.validate$churn <- as.factor(ifelse(churn.dec.validate$churn==1,"yes","no"))
```

#Using CrossTable() to look at the performance metrics and miscalculations for all the models

```{r}
#Logistic Regression Model
CrossTable(x=churn.lr.validate$churn,y=churn.lr.validate$prob,prop.chisq = F)
```
###Performance Metrics - Logistic Regression Model###\vspace{.5mm}
\newline
#True Positive (TP) - 13#\vspace{.5mm}
\newline
#True Negative (TN) - 712#\vspace{.5mm}
\newline
#False Positive (FP) - 0#\vspace{.5mm}
\newline
#False Negative (FN) - 107#\vspace{.5mm}
\newline
#Miscalculations - 107#\vspace{1mm}
\newline
#Accuracy = TP+TN/TP+TN+FP+FN = 13+712/832 = 87.13 %#\vspace{.5mm}
\newline
#Specificity (TNR) = TN/TN+FP = 712/712+0 = 100 %#\vspace{.5mm}
\newline
#Sensitivity (TPR) = TP/TP+FN = 13/13+107 = 10.83 %#\vspace{1mm}
\newline

```{r}
#Decision Tree Model
CrossTable(x=churn.dec.validate$churn,y=churn.dec.validate$prob,prop.chisq = F)
```

###Performance Metrics - Decision Tree###\vspace{.5mm}
\newline
#True Positive (TP) - 92#\vspace{.5mm}
\newline
#True Negative (TN) - 690#\vspace{.5mm}
\newline
#False Positive (FP) - 22#\vspace{.5mm}
\newline
#False Negative (FN) - 28#\vspace{.5mm}
\newline
#Miscalculations - 50#\vspace{1mm}
\newline
#Accuracy = TP+TN/TP+TN+FP+FN = 92+690/832 = 93.99 %#\vspace{.5mm}
\newline
#Specificity (TNR) = TN/TN+FP = 690/690+22 = 96.91 %#\vspace{.5mm}
\newline
#Sensitivity (TPR) = TP/TP+FN = 92/92+28 = 76.66 %#\vspace{1mm}
\newline
###Eventually, we can see that the decision tree model is working quite good on the validation set when compared to that with the other models. Accuracy, Sensitivity and Specificity is comparatively high so we are proceeding with the decision tree model to be implemented on the "test set".###\vspace{2mm}
\newline
#In order to use an effective model on the test set we did try to use pruning as well to check if there's any rise in the accuracy#\vspace{1mm}
\newline


#Pruning the decision tree model

```{r}
#Base Model
#The Dec_Tree.model is the base model which was already built at the beginning
printcp(Dec_Tree.model)
plotcp(Dec_Tree.model)
#The base model accuracy as seen above is 93.99% (94% approx)

# Pre-Pruning
# Growing a tree with minsplit of 50 and maxdepth of 6
Dec_Tree.model_preprun <- rpart(churn ~ ., data = Train_Data, method = "class", control = rpart.control(cp=0,minsplit = 50,maxdepth = 6))

# predicting the above pre-pruned tree on the validation set
churn.dec.validate.preprun <- predict(Dec_Tree.model_preprun, Validation_Data, type = "prob")
churn.dec.validate.preprun.df <- cbind(Validation_Data,churn.dec.validate.preprun)

#Optimal K
ROC_pred_dec.pre_test <- prediction(churn.dec.validate.preprun[,1],churn.dec.validate.preprun.df$churn)

ROCR_perf_dec.pre_test <- performance(ROC_pred_dec.pre_test,'tpr','fpr')

acc_dec.pre_perf <- performance(ROC_pred_dec.pre_test,"acc")

ROC_pred_dec.pre_test@cutoffs[[1]][which.max(acc_dec.pre_perf@y.values[[1]])]

#AUC Value
roc.curve(churn.dec.validate.preprun.df$churn,churn.dec.validate.preprun[,1], plotit = F)

#Calculating Accuracy
churn.dec.validate.preprun.df$prob <- as.factor(ifelse(churn.dec.validate.preprun.df$`1`>0.7857143,1,0))

accuracy_preprun <- mean(churn.dec.validate.preprun.df$churn==churn.dec.validate.preprun.df$prob)
accuracy_preprun

#Post- Pruning
# Pruning the Dec_Tree.model based on the optimal cp value
Dec_tree.model_pruned <- prune(Dec_Tree.model, cp = 0.0100)

#predicting the above pruned tree on the validation set
churn.dec.validate.pruned <- predict(Dec_tree.model_pruned, Validation_Data, type = "prob")
churn.dec.validate.pruned.df <- cbind(Validation_Data,churn.dec.validate.pruned)

#Optimal K
ROC_pred_dec.pos_test <- prediction(churn.dec.validate.pruned[,1],churn.dec.validate.pruned.df$churn)

ROCR_perf_dec.pos_test <- performance(ROC_pred_dec.pos_test,'tpr','fpr')

acc_dec.pos_perf <- performance(ROC_pred_dec.pos_test,"acc")

ROC_pred_dec.pos_test@cutoffs[[1]][which.max(acc_dec.pos_perf@y.values[[1]])]

#AUC Value
roc.curve(churn.dec.validate.pruned.df$churn,churn.dec.validate.pruned[,1], plotit = F)

#Calculating Accuracy
churn.dec.validate.pruned.df$prob <- as.factor(ifelse(churn.dec.validate.pruned.df$`1`>0.3076923,1,0))

accuracy_postprun <- mean(churn.dec.validate.pruned.df$churn==churn.dec.validate.pruned.df$prob)
accuracy_postprun

#Comparing the base mode, pre-pruning model and post pruning model's accuracy
#Base model accuracy = 0.9399038
data.frame(accuracy_preprun, accuracy_postprun)
```
#Pruning can not have significant impact when the data is imbalanced and this can be a possible reason to not see any change in the accuracy in "post - pruning model". We are thereby affirming to the base model and using the base model (Dec_Tree_Model) to predict the test set.#\vspace{2mm}
\newline

#Prediction on the test set 

```{r}
#Re-coding the variables as being used in the train set
Customers_To_Predict$international_plan <- ifelse(Customers_To_Predict$international_plan =="yes",1,0)
Customers_To_Predict$voice_mail_plan <- ifelse(Customers_To_Predict$voice_mail_plan =="yes",1,0)

#Predicting the decision tree model built over the unseen data
dec.test <- predict(Dec_Tree.model,Customers_To_Predict,type="prob")
churn.dec.test <- cbind(Customers_To_Predict,dec.test)

#Setting the baseline model cutoff point i.e. 0.3076923 on the test set
churn.dec.test$prob <- as.factor(ifelse(churn.dec.test$`1`>0.3076923,"yes","no"))

#Deleting the probability columns 1 and 0
churn.dec.test <- churn.dec.test[,-c(20:21)]
```

*The final file to look for the churns and no churns is the churn.dec.test*
